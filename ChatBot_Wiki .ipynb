{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d060b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries \n",
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import nltk\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import numpy as np\n",
    "from time import sleep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a178d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sowmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9503670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sowmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1062fd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sowmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09ccfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot():\n",
    "    \n",
    "    # initialize bot\n",
    "    def __init__(self):\n",
    "        # flag whether to end chat\n",
    "        self.end_chat = False\n",
    "        # flag whether topic is found in wikipedia\n",
    "        self.got_topic = False\n",
    "        # flag whether to call respond()\n",
    "        # in some cases, response be made already\n",
    "        self.do_not_respond = True\n",
    "        \n",
    "        # wikipedia title\n",
    "        self.title = None\n",
    "        # wikipedia scraped para and description data\n",
    "        self.text_data = []\n",
    "        # data as sentences\n",
    "        self.sentences = []\n",
    "        # to keep track of paragraph indices\n",
    "        # corresponding to all sentences\n",
    "        self.para_indices = []\n",
    "        # currently retrieved sentence id\n",
    "        self.current_sent_idx = None\n",
    "        \n",
    "        # a punctuation dictionary\n",
    "        self.punctuation_dict = str.maketrans({p:None for p in punctuation})\n",
    "        # wordnet lemmatizer for preprocessing text\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        # collection of stopwords\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        # initialize chatting\n",
    "        self.greeting()\n",
    "\n",
    "    # greeting method - to be called internally\n",
    "    # chatbot initializing chat on screen with greetings\n",
    "    def greeting(self):\n",
    "        print(\"Initializing ChatBot ...\")\n",
    "        # some time to get user ready\n",
    "        sleep(5)\n",
    "        # chat ending tags\n",
    "        print('Type \"Bye\" or \"Quit\" or \"Exit\" to end chat')\n",
    "        sleep(5)\n",
    "        # chatbot descriptions\n",
    "        print('\\nIn box enter your topic of interest. \\\n",
    "        \\nChaBot will access Wikipedia, prepare itself and \\\n",
    "        \\nrespond to your queries on the topic. \\n')\n",
    "        sleep(5)\n",
    "        print('ChatBot will respond with short info. \\\n",
    "        \\nIf you input \"more\", it will give you detailed info \\\n",
    "        \\nYou can also jump to next query')\n",
    "        # give time to read what has been printed\n",
    "        sleep(5)\n",
    "        print('-'*50)\n",
    "        # Greet and introduce\n",
    "        greet = \"Hello, Please give me a topic. \"\n",
    "        print(\"ChatBot: \" + greet)\n",
    "        \n",
    "    # chat method - should be called by user\n",
    "    # to do the entire chatting on one go!\n",
    "    def chat(self):\n",
    "        # continue chat\n",
    "        while not self.end_chat:\n",
    "            # receive input\n",
    "            self.receive_input()\n",
    "            # finish chat if opted by user\n",
    "            if self.end_chat:\n",
    "                print('ChatBot: See you soon! Bye!')\n",
    "                sleep(2)\n",
    "                print('\\nQuitting ChatBot ...')\n",
    "            # if data scraping successful\n",
    "            elif self.got_topic:\n",
    "                # in case not already responded\n",
    "                if not self.do_not_respond:\n",
    "                    self.respond()\n",
    "                # clear flag so that bot can respond next time\n",
    "                self.do_not_respond = False\n",
    "    \n",
    "    # receive_input method - to be called internally\n",
    "    # recieves input from user and makes preliminary decisions\n",
    "    def receive_input(self):\n",
    "        # receive input from user\n",
    "        text = input(\"User: \")\n",
    "        # end conversation if user wishes so\n",
    "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
    "            # turn flag on \n",
    "            self.end_chat=True\n",
    "        # if user needs more information \n",
    "        elif text.lower().strip() == 'more':\n",
    "            # respond here itself\n",
    "            self.do_not_respond = True\n",
    "            # if at least one query has been received \n",
    "            if self.current_sent_idx != None:\n",
    "                response = self.text_data[self.para_indices[self.current_sent_idx]]\n",
    "            # prompt user to start querying\n",
    "            else:\n",
    "                response = \"Please input your query first!\"\n",
    "            print(\"ChatBot: \" + response)\n",
    "        # if topic is not chosen\n",
    "        elif not self.got_topic:\n",
    "            self.scrape_wiki(text)\n",
    "        else:\n",
    "            # add user input to sentences, so that we can vectorize in whole\n",
    "            self.sentences.append(text)\n",
    "                \n",
    "    # respond method - to be called internally\n",
    "    def respond(self):\n",
    "        # tf-idf-modeling\n",
    "        vectorizer = TfidfVectorizer(tokenizer=self.preprocess)\n",
    "        # fit data and obtain tf-idf vector\n",
    "        tfidf = vectorizer.fit_transform(self.sentences)\n",
    "        # calculate cosine similarity scores\n",
    "        scores = cosine_similarity(tfidf[-1],tfidf) \n",
    "        # identify the most closest sentence\n",
    "        self.current_sent_idx = scores.argsort()[0][-2]\n",
    "        # find the corresponding score value\n",
    "        scores = scores.flatten()\n",
    "        scores.sort()\n",
    "        value = scores[-2]\n",
    "        # if there is matching sentence\n",
    "        if value != 0:\n",
    "            print(\"ChatBot: \" + self.sentences[self.current_sent_idx]) \n",
    "        # if no sentence is matching the query\n",
    "        else:\n",
    "            print(\"ChatBot: I am not sure. Sorry!\" )\n",
    "        # remove the user query from sentences\n",
    "        del self.sentences[-1]\n",
    "        \n",
    "    # scrape_wiki method - to be called internally.\n",
    "    def scrape_wiki(self,topic):\n",
    "        # process topic as required by Wikipedia URL system\n",
    "        topic = topic.lower().strip().capitalize().split(' ')\n",
    "        topic = '_'.join(topic)\n",
    "        try:\n",
    "            # creata an url\n",
    "            link = 'https://en.wikipedia.org/wiki/'+ topic\n",
    "            # access contents via url\n",
    "            data = requests.get(link).content\n",
    "            # parse data as soup object\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            # extract all paragraph data\n",
    "            # scrape strings with html tag 'p'\n",
    "            p_data = soup.findAll('p')\n",
    "            # scrape strings with html tag 'dd'\n",
    "            dd_data = soup.findAll('dd')\n",
    "            # scrape strings with html tag 'li'\n",
    "            #li_data = soup.findAll('li')\n",
    "            p_list = [p for p in p_data]\n",
    "            dd_list = [dd for dd in dd_data]\n",
    "            #li_list = [li for li in li_data]\n",
    "            # iterate over all data\n",
    "            for tag in p_list+dd_list: #+li_list:\n",
    "                # a bucket to collect processed data\n",
    "                a = []\n",
    "                # iterate over para, desc data and list items contents\n",
    "                for i in tag.contents:\n",
    "                    # exclude references, superscripts, formattings\n",
    "                    if i.name != 'sup' and i.string != None:\n",
    "                        stripped = ' '.join(i.string.strip().split())\n",
    "                        # collect data pieces\n",
    "                        a.append(stripped)\n",
    "                # with collected string pieces formulate a single string\n",
    "                # each string is a paragraph\n",
    "                self.text_data.append(' '.join(a))\n",
    "            \n",
    "            # obtain sentences from paragraphs\n",
    "            for i,para in enumerate(self.text_data):\n",
    "                sentences = nltk.sent_tokenize(para)\n",
    "                self.sentences.extend(sentences)\n",
    "                # for each sentence, its para index must be known\n",
    "                # it will be useful in case user prompts \"more\" info\n",
    "                index = [i]*len(sentences)\n",
    "                self.para_indices.extend(index)\n",
    "            \n",
    "            # extract h1 heading tag from soup object\n",
    "            self.title = soup.find('h1').string\n",
    "            # turn respective flag on\n",
    "            self.got_topic = True\n",
    "            # announce user that chatbot is ready now\n",
    "            print('ChatBot: Topic is \"Wikipedia: {}\". Let\\'s chat!'.format(self.title)) \n",
    "        # in case of unavailable topics\n",
    "        except Exception as e:\n",
    "            print('ChatBot: Error: {}. \\\n",
    "            Please input some other topic!'.format(e))\n",
    "        \n",
    "    # preprocess method - to be called internally by Tf-Idf vectorizer\n",
    "    # text preprocessing, stopword removal, lemmatization, word tokenization\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower().strip().translate(self.punctuation_dict) \n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [w for w in words if w not in self.stopwords]\n",
    "        return [self.lemmatizer.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91f05a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChatBot ...\n",
      "Type \"Bye\" or \"Quit\" or \"Exit\" to end chat\n",
      "\n",
      "In box enter your topic of interest.         \n",
      "ChaBot will access Wikipedia, prepare itself and         \n",
      "respond to your queries on the topic. \n",
      "\n",
      "ChatBot will respond with short info.         \n",
      "If you input \"more\", it will give you detailed info         \n",
      "You can also jump to next query\n",
      "--------------------------------------------------\n",
      "ChatBot: Hello, Please give me a topic. \n",
      "User: water \n",
      "ChatBot: Topic is \"Wikipedia: Water\". Let's chat!\n",
      "User: how much water is there on earth\n",
      "ChatBot: The majority of water on Earth is sea water .\n",
      "User: more\n",
      "ChatBot: Liquid water is found in bodies of water , such as an ocean, sea, lake, river, stream, canal , pond, or puddle . The majority of water on Earth is sea water . Water is also present in the atmosphere in solid, liquid, and vapor states. It also exists as groundwater in aquifers .\n",
      "User: exit\n",
      "ChatBot: See you soon! Bye!\n",
      "\n",
      "Quitting ChatBot ...\n"
     ]
    }
   ],
   "source": [
    "chat = ChatBot()\n",
    "chat.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
